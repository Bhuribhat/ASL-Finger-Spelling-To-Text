{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thai Hand Gesture Recognition\n",
    "\n",
    "<!-- ![numbers](./assets/numbers.png) -->\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/numbers.png\" width=\"90%\">\n",
    "</p>\n",
    "\n",
    "## Pipeline\n",
    "To implement hand gesture recognition, there are `3` step which are shown below:\n",
    "\n",
    "- **Hand Detection**: `MediaPipe`\n",
    "  - To detect hands in frame\n",
    "- **Hand Tracking**: `Algorithm`\n",
    "  - To handle multiple hands in the same time\n",
    "- **Gesture Recognition**: `Algorithm`\n",
    "  - To overcome amount of gesture limitation \n",
    "  - To recognize gesture\n",
    "  - To handle both `right-hand` and `left-hand`\n",
    "\n",
    "## Hand Detection\n",
    "Fotunately, [MediaPipe](https://mediapipe.dev/) has provided `MediaPipe Hands` which not only can detect hand in frame but also return the coordinate `x`, `y` and `z` of each `21 landmarks` on each hand in frame. Furthermore, it can also detect that whether the hand in frame is `right` or `left` hand.\n",
    "\n",
    "In the [original project](https://github.com/NatthanonNon/HGR-TH/tree/main), They used `Python Solution API` as a hand detection, you can read more details in [their website](https://google.github.io/mediapipe/solutions/hands).\n",
    "\n",
    "## Hand Tracking\n",
    "There are only `3` events that can happen in each frame: the numbers of hand in frame is `increasing`, `decreasing` or `equaling`. So, in each events, we do the different process.\n",
    "\n",
    "The main idea of this algorithm is comparing the center of box of each hand between the `previous frame` and the `present frame`. And using `Euclidean Distance` to compare them.\n",
    "\n",
    "## Gesture Recognition\n",
    "As `MediaPipe Hands` returns the coordinate `x`, `y` and `z` of each `21 landmarks` on each hand in frame, only coordinate `x` and `y` are used to recognize the gesture because the `z` is not robust enough.\n",
    "\n",
    "There are `2` step in this process:\n",
    "- 1. Use coordinate `x` and `y` to identify the `status` of each finger whether it is `on` or `off`\n",
    "- 2. Use the status of each finger to identify the gesture by following the `Hand Gestures Definition`\n",
    "\n",
    "To handle more than one digit, `timing` is brought into play. And using `threshold` to indicate that the recognized gesture is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependacies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from utils import calc_landmark_list, draw_landmarks, draw_info_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors RGB Format\n",
    "BLACK  = (0, 0, 0)\n",
    "RED    = (255, 0, 0)\n",
    "GREEN  = (0, 255, 0)\n",
    "BLUE   = (0, 0, 255)\n",
    "YELLOW = (0, 255, 255)\n",
    "WHITE  = (255, 255, 255)\n",
    "\n",
    "# Constants\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "TIMING = 10                     #@param {type: \"integer\"}\n",
    "MAX_HANDS = 1                   #@param {type: \"integer\"}\n",
    "min_detection_confidence = 0.6  #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "min_tracking_confidence  = 0.5  #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "\n",
    "current_hand = 0\n",
    "gif_array = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_landmark(landmark):\n",
    "    wrist = landmark[0]\n",
    "    thump = landmark[1:5]\n",
    "    index_finger = landmark[5:9]\n",
    "    middle_finger = landmark[9:13]\n",
    "    ring_finger = landmark[13:17]\n",
    "    pinky = landmark[17:21]\n",
    "    return [wrist, thump, index_finger, middle_finger, ring_finger, pinky]\n",
    "\n",
    "\n",
    "def is_on(idx, finger, landmark_label):\n",
    "    if idx == 0:\n",
    "        if landmark_label == \"Right\":\n",
    "            return finger[-1].x < finger[-2].x\n",
    "        else:\n",
    "            return finger[-1].x > finger[-2].x\n",
    "    return finger[-1].y < finger[0].y\n",
    "\n",
    "\n",
    "def gesture_recognition(finger_is_on):\n",
    "    thump = finger_is_on[0]\n",
    "    index_finger = finger_is_on[1]\n",
    "    middle_finger = finger_is_on[2]\n",
    "    ring_finger = finger_is_on[3]\n",
    "    pinky = finger_is_on[4]\n",
    "    \n",
    "    # Thai Digits Hand Gesture\n",
    "    if thump:\n",
    "        if index_finger and middle_finger and ring_finger and pinky:\n",
    "            return 5\n",
    "        elif index_finger and middle_finger and ring_finger and not pinky:\n",
    "            return 9\n",
    "        elif index_finger and middle_finger and not ring_finger and not pinky:\n",
    "            return 8\n",
    "        elif index_finger and not middle_finger and not ring_finger and not pinky:\n",
    "            return 7\n",
    "        elif not index_finger and not middle_finger and not ring_finger and not pinky:\n",
    "            return 6\n",
    "    else:\n",
    "        if index_finger and middle_finger and ring_finger and pinky:\n",
    "            return 4\n",
    "        elif index_finger and middle_finger and ring_finger and not pinky:\n",
    "            return 3\n",
    "        elif index_finger and middle_finger and not ring_finger and not pinky:\n",
    "            return 2\n",
    "        elif index_finger and not middle_finger and not ring_finger and not pinky:\n",
    "            return 1\n",
    "        elif not index_finger and not middle_finger and not ring_finger and not pinky:\n",
    "            return 0\n",
    "\n",
    "    # Unknown Gesture\n",
    "    return \"?\"\n",
    "\n",
    "\n",
    "def recognition(landmark, handness):\n",
    "    landmark = landmark.landmark\n",
    "    handness = handness.classification[0].label\n",
    "    \n",
    "    hand_landmark = classify_landmark(landmark)\n",
    "    finger_landmark = hand_landmark[1:]\n",
    "    \n",
    "    finger_is_on = []\n",
    "    for idx, finger in enumerate(finger_landmark):\n",
    "        finger_is_on.append(is_on(idx, finger, handness))\n",
    "\n",
    "    return gesture_recognition(finger_is_on)\n",
    "\n",
    "\n",
    "def get_output(idx):\n",
    "    global _output, output\n",
    "    key = []\n",
    "    for i in range(len(_output[idx])):\n",
    "        number = _output[idx][i]\n",
    "        counts = _output[idx].count(number)\n",
    "\n",
    "        # Add number to key if exceed 'TIMING THRESHOLD'\n",
    "        if number not in key:\n",
    "            if counts > TIMING:\n",
    "                key.append(number)\n",
    "\n",
    "        # Handle duplicate numbers\n",
    "        elif number != key[-1]:\n",
    "            if counts > TIMING:\n",
    "                key.append(number)\n",
    "\n",
    "    # Add key number to output text\n",
    "    text = \"\"\n",
    "    for number in key:\n",
    "        if number == \"?\":\n",
    "            continue\n",
    "        text += str(number)\n",
    "\n",
    "    # Add word to output list\n",
    "    if text != \"\":\n",
    "        _output[idx] = []\n",
    "        output.append(text)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "\n",
    "def find_vanishing(_mean_xy):\n",
    "    global mean_xy\n",
    "    \n",
    "    _a = get_euclidean_distance(mean_xy[0], _mean_xy[0])\n",
    "    _b = get_euclidean_distance(mean_xy[1], _mean_xy[0])\n",
    "    \n",
    "    if _a > _b:\n",
    "        mean_xy[0] = mean_xy[1]\n",
    "        mean_xy[1] = []\n",
    "        return 0\n",
    "    else:\n",
    "        mean_xy[1] = []\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(image, results):\n",
    "    global mp_drawing, current_hand \n",
    "    global output, _output\n",
    "    global mean_xy\n",
    "\n",
    "    multi_hand_landmarks = results.multi_hand_landmarks\n",
    "    multi_handedness = results.multi_handedness\n",
    "\n",
    "    _mean_xy = []\n",
    "    _gesture = []\n",
    "\n",
    "    isIncreased = False\n",
    "    isDecreased = False\n",
    "\n",
    "    if current_hand != 0:\n",
    "        if results.multi_hand_landmarks is None:\n",
    "            isDecreased = True\n",
    "        else:\n",
    "            if len(multi_hand_landmarks) > current_hand:\n",
    "                isIncreased = True\n",
    "            elif len(multi_hand_landmarks) < current_hand:\n",
    "                isDecreased = True\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        h, w, _ = image.shape\n",
    "        for idx in reversed(range(len(multi_hand_landmarks))):\n",
    "            current_select_hand = multi_hand_landmarks[idx]\n",
    "\n",
    "            # mp_drawing.draw_landmarks(image, current_select_hand, mp_hands.HAND_CONNECTIONS)\n",
    "            landmark_list = calc_landmark_list(image, current_select_hand)\n",
    "            image = draw_landmarks(image, landmark_list)\n",
    "\n",
    "            min_x = int(min([current_select_hand.landmark[i].x for i in range(len(current_select_hand.landmark))]) * w)\n",
    "            max_x = int(max([current_select_hand.landmark[i].x for i in range(len(current_select_hand.landmark))]) * w)\n",
    "            min_y = int(min([current_select_hand.landmark[i].y for i in range(len(current_select_hand.landmark))]) * h)\n",
    "            max_y = int(max([current_select_hand.landmark[i].y for i in range(len(current_select_hand.landmark))]) * h)\n",
    "\n",
    "            # Drawing Bounding Box\n",
    "            cv2.rectangle(image, (min_x - 10, min_y - 10), (max_x + 10, max_y + 10), BLACK, 2)\n",
    "            gesture = recognition(current_select_hand, multi_handedness[idx])\n",
    "\n",
    "            order_text = \"Hand No. {}\".format(idx)\n",
    "            cv2.putText(image, order_text, (min_x - 10, max_y + 30), FONT, 0.5, GREEN, 2)\n",
    "\n",
    "            # gesture_text = \"Gesture: {}\".format(gesture)\n",
    "            cv2.rectangle(image, (min_x - 10, min_y - 10), (max_x + 10, max_y + 10), BLACK, 4)\n",
    "            image = draw_info_text(image, [min_x - 10, min_y - 10, max_x + 10, max_y + 10], str(gesture))\n",
    "\n",
    "            handness_text = \"{} hand\".format(multi_handedness[idx].classification[0].label)\n",
    "            cv2.putText(image, handness_text, (min_x - 10, max_y + 60), FONT, 0.5, GREEN, 2)\n",
    "\n",
    "            _mean_xy.append(np.array([(min_x + max_x) / 2, (min_y + max_y) / 2]))\n",
    "            _gesture.append(gesture)\n",
    "\n",
    "    # Number of hands is increasing\n",
    "    if isIncreased == True:\n",
    "        mean_xy[0] = _mean_xy[0]\n",
    "        if current_hand == 1:  \n",
    "            mean_xy[1] = _mean_xy[1]\n",
    "\n",
    "    # Number of hands is decreasing\n",
    "    elif isDecreased == True:\n",
    "        if current_hand == 1:\n",
    "            get_output(0)\n",
    "        elif current_hand == 2:\n",
    "            vanishing_index = find_vanishing(_mean_xy)\n",
    "            get_output(vanishing_index)\n",
    "\n",
    "    # Number of hands is the same\n",
    "    else:\n",
    "        if results.multi_hand_landmarks is not None:\n",
    "            mean_xy[0] = _mean_xy[0]\n",
    "            _output[0].append(_gesture[0])\n",
    "            \n",
    "            if current_hand == 2:\n",
    "                mean_xy[1] = _mean_xy[1]\n",
    "                _output[1].append(_gesture[1])\n",
    "\n",
    "    # Track hand numbers\n",
    "    if results.multi_hand_landmarks:\n",
    "        current_hand = len(multi_hand_landmarks)\n",
    "    else:\n",
    "        current_hand = 0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = []\n",
    "_output = [[], []]\n",
    "mean_xy = [[], []]\n",
    "\n",
    "# Webcam Input\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=min_detection_confidence,\n",
    "    min_tracking_confidence=min_tracking_confidence, \n",
    "    max_num_hands=MAX_HANDS\n",
    ") as hands:\n",
    "    while capture.isOpened():\n",
    "        success, image = capture.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display, and convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # To improve performance, optionally mark the image as not writeable to pass by reference\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            image = main(image, results)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "        # Show output in Top-Left corner\n",
    "        number_text = str(output)\n",
    "        textsize = cv2.getTextSize(number_text, FONT, 0.5, 2)[0]\n",
    "        cv2.rectangle(image, (5, 0), (10 + textsize[0], 10 + textsize[1]), YELLOW, -1)\n",
    "        cv2.putText(image, number_text, (10, 15), FONT, 0.5, BLACK, 2)\n",
    "        cv2.imshow('Number Recognition', image)\n",
    "\n",
    "        # Save each frames to GIF\n",
    "        gif_array.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Press 'Esc' to quit\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            clear_output()\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesture Recognition:\n",
      "['0', '4', '2541', '2451', '1', '7', '60', '3605']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gesture Recognition:\\n{number_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate GIF Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to ./assets/result_number.gif!\n"
     ]
    }
   ],
   "source": [
    "from utils import save_gif\n",
    "\n",
    "fps = 30    #@param {type: \"integer\"}\n",
    "save_gif(\n",
    "    gif_array, fps=fps, \n",
    "    output_dir=\"./assets/result_number.gif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of the Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35eef248e71f5de021830aa77e0d2f7d183198295a21f1397f2503c4fa0b8008"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
